{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38704237",
   "metadata": {},
   "source": [
    "# Batch Evaluation of Llama 3.2 Vision on Safaitic Inscriptions\n",
    "\n",
    "This notebook runs systematic evaluations across multiple inscriptions to:\n",
    "1. Test VLM performance at scale\n",
    "2. Generate aggregate statistics\n",
    "3. Identify strengths and weaknesses\n",
    "4. Export results for analysis\n",
    "\n",
    "**Note**: This can take significant time depending on the number of inscriptions tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12af6de",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src import (\n",
    "    load_metadata,\n",
    "    get_inscription_data,\n",
    "    list_available_inscriptions,\n",
    "    LlamaVision,\n",
    "    SafaiticPrompts,\n",
    "    InscriptionEvaluator\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba11583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "llama = LlamaVision(model_name=\"llama3.2-vision\", timeout=180)\n",
    "prompts = SafaiticPrompts()\n",
    "evaluator = InscriptionEvaluator()\n",
    "\n",
    "# Verify model availability\n",
    "if not llama.check_availability():\n",
    "    raise RuntimeError(\"Llama 3.2 Vision not available. Cannot proceed.\")\n",
    "\n",
    "print(\"✓ Model ready for batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6922c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = project_root / \"metadata\" / \"BES15.csv\"\n",
    "data_dir = project_root / \"data\"\n",
    "df = load_metadata(str(metadata_path))\n",
    "\n",
    "available_inscriptions = list_available_inscriptions(df)\n",
    "print(f\"Total available inscriptions: {len(available_inscriptions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64794baf",
   "metadata": {},
   "source": [
    "## Configure Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Select inscriptions to test\n",
    "    'inscription_ids': available_inscriptions[:10],  # Start with first 10\n",
    "    \n",
    "    # Which prompt template to use\n",
    "    'prompt_template': 'transliteration_attempt',  # Options: see SafaiticPrompts.get_all_prompts()\n",
    "    \n",
    "    # Image selection per inscription\n",
    "    'images_per_inscription': 1,  # How many images to analyze per inscription (use -1 for all)\n",
    "    'image_selection': 'first',  # 'first', 'random', or 'all'\n",
    "    \n",
    "    # Model parameters\n",
    "    'temperature': 0.1,\n",
    "    \n",
    "    # Output\n",
    "    'save_results': True,\n",
    "    'results_dir': project_root / 'results',\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d4232",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c133da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inscription(inscription_id: str, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single inscription.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load inscription data\n",
    "        inscription = get_inscription_data(df, str(data_dir), inscription_id, load_images=True)\n",
    "        \n",
    "        if not inscription.images:\n",
    "            return {\n",
    "                'inscription_id': inscription_id,\n",
    "                'status': 'error',\n",
    "                'error': 'No images found',\n",
    "            }\n",
    "        \n",
    "        # Select images to test\n",
    "        if config['image_selection'] == 'first':\n",
    "            test_images = inscription.images[:config['images_per_inscription']]\n",
    "        elif config['image_selection'] == 'all':\n",
    "            test_images = inscription.images\n",
    "        else:  # random\n",
    "            import random\n",
    "            n = min(config['images_per_inscription'], len(inscription.images))\n",
    "            test_images = random.sample(inscription.images, n)\n",
    "        \n",
    "        # Get prompt\n",
    "        prompt_method = getattr(prompts, config['prompt_template'])\n",
    "        prompt = prompt_method()\n",
    "        \n",
    "        # Analyze images\n",
    "        image_results = []\n",
    "        for idx, image in enumerate(test_images):\n",
    "            result = llama.analyze_image(\n",
    "                image,\n",
    "                prompt,\n",
    "                temperature=config['temperature']\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                # Evaluate the response\n",
    "                evaluation = evaluator.evaluate_inscription(\n",
    "                    inscription.transliteration,\n",
    "                    inscription.translation,\n",
    "                    result['response']\n",
    "                )\n",
    "                \n",
    "                # Add script identification check\n",
    "                script_check = evaluator.check_script_identification(result['response'])\n",
    "                \n",
    "                image_results.append({\n",
    "                    'image_index': idx,\n",
    "                    'success': True,\n",
    "                    'response': result['response'],\n",
    "                    'evaluation': evaluation,\n",
    "                    'script_identification': script_check,\n",
    "                    'duration': result.get('total_duration'),\n",
    "                })\n",
    "            else:\n",
    "                image_results.append({\n",
    "                    'image_index': idx,\n",
    "                    'success': False,\n",
    "                    'error': result.get('error'),\n",
    "                })\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        successful_evals = [r['evaluation'] for r in image_results if r['success']]\n",
    "        \n",
    "        if successful_evals:\n",
    "            avg_score = np.mean([e['overall_score'] for e in successful_evals])\n",
    "            avg_char_sim = np.mean([e['character_metrics']['similarity_ratio'] for e in successful_evals])\n",
    "            avg_word_acc = np.mean([e['word_metrics']['word_accuracy'] for e in successful_evals])\n",
    "        else:\n",
    "            avg_score = avg_char_sim = avg_word_acc = 0\n",
    "        \n",
    "        return {\n",
    "            'inscription_id': inscription_id,\n",
    "            'status': 'success',\n",
    "            'ground_truth': {\n",
    "                'transliteration': inscription.transliteration,\n",
    "                'translation': inscription.translation,\n",
    "                'transliteration_length': len(inscription.transliteration),\n",
    "            },\n",
    "            'num_images_tested': len(test_images),\n",
    "            'image_results': image_results,\n",
    "            'aggregate_metrics': {\n",
    "                'avg_overall_score': avg_score,\n",
    "                'avg_character_similarity': avg_char_sim,\n",
    "                'avg_word_accuracy': avg_word_acc,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'inscription_id': inscription_id,\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "        }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80778b90",
   "metadata": {},
   "source": [
    "## Run Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faad4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting evaluation of {len(CONFIG['inscription_ids'])} inscriptions...\\n\")\n",
    "\n",
    "for inscription_id in tqdm(CONFIG['inscription_ids'], desc=\"Evaluating\"):\n",
    "    result = evaluate_inscription(inscription_id, CONFIG)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Brief pause between inscriptions to avoid overwhelming the system\n",
    "    time.sleep(0.5)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n✓ Evaluation complete in {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Average time per inscription: {elapsed_time/len(results):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce39f",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "successful_results = [r for r in results if r['status'] == 'success']\n",
    "failed_results = [r for r in results if r['status'] == 'error']\n",
    "\n",
    "print(f\"Results Summary:\")\n",
    "print(f\"  Total inscriptions: {len(results)}\")\n",
    "print(f\"  Successful: {len(successful_results)}\")\n",
    "print(f\"  Failed: {len(failed_results)}\")\n",
    "\n",
    "if failed_results:\n",
    "    print(\"\\nFailed inscriptions:\")\n",
    "    for r in failed_results:\n",
    "        print(f\"  - {r['inscription_id']}: {r['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "if successful_results:\n",
    "    scores = [r['aggregate_metrics']['avg_overall_score'] for r in successful_results]\n",
    "    char_sims = [r['aggregate_metrics']['avg_character_similarity'] for r in successful_results]\n",
    "    word_accs = [r['aggregate_metrics']['avg_word_accuracy'] for r in successful_results]\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Overall Score:\")\n",
    "    print(f\"    Mean: {np.mean(scores):.3f}\")\n",
    "    print(f\"    Median: {np.median(scores):.3f}\")\n",
    "    print(f\"    Std Dev: {np.std(scores):.3f}\")\n",
    "    print(f\"    Range: {np.min(scores):.3f} - {np.max(scores):.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Character Similarity:\")\n",
    "    print(f\"    Mean: {np.mean(char_sims):.3f}\")\n",
    "    print(f\"    Median: {np.median(char_sims):.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Word Accuracy:\")\n",
    "    print(f\"    Mean: {np.mean(word_accs):.3f}\")\n",
    "    print(f\"    Median: {np.median(word_accs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script identification accuracy\n",
    "if successful_results:\n",
    "    script_identifications = []\n",
    "    for r in successful_results:\n",
    "        for img_result in r['image_results']:\n",
    "            if img_result['success'] and 'script_identification' in img_result:\n",
    "                script_identifications.append(img_result['script_identification'])\n",
    "    \n",
    "    if script_identifications:\n",
    "        correct_ids = sum(1 for s in script_identifications if s['correctly_identified'])\n",
    "        total_ids = len(script_identifications)\n",
    "        \n",
    "        print(f\"\\nScript Identification:\")\n",
    "        print(f\"  Correctly identified as Safaitic: {correct_ids}/{total_ids} ({100*correct_ids/total_ids:.1f}%)\")\n",
    "        print(f\"  Average confidence: {np.mean([s['confidence'] for s in script_identifications]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17b5e8",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6db77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_results:\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Score distribution\n",
    "    axes[0, 0].hist(scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')\n",
    "    axes[0, 0].set_xlabel('Overall Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Overall Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Performance by inscription\n",
    "    inscr_ids = [r['inscription_id'] for r in successful_results]\n",
    "    inscr_scores = [r['aggregate_metrics']['avg_overall_score'] for r in successful_results]\n",
    "    \n",
    "    axes[0, 1].barh(range(len(inscr_ids)), inscr_scores, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].set_yticks(range(len(inscr_ids)))\n",
    "    axes[0, 1].set_yticklabels(inscr_ids, fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Overall Score')\n",
    "    axes[0, 1].set_title('Performance by Inscription')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Character vs Word accuracy\n",
    "    axes[1, 0].scatter(char_sims, word_accs, alpha=0.6, s=100)\n",
    "    axes[1, 0].set_xlabel('Character Similarity')\n",
    "    axes[1, 0].set_ylabel('Word Accuracy')\n",
    "    axes[1, 0].set_title('Character Similarity vs Word Accuracy')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect correlation')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Score vs transliteration length\n",
    "    translit_lengths = [len(r['ground_truth']['transliteration']) for r in successful_results]\n",
    "    axes[1, 1].scatter(translit_lengths, inscr_scores, alpha=0.6, s=100)\n",
    "    axes[1, 1].set_xlabel('Transliteration Length (characters)')\n",
    "    axes[1, 1].set_ylabel('Overall Score')\n",
    "    axes[1, 1].set_title('Performance vs Inscription Length')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd97060",
   "metadata": {},
   "source": [
    "## Identify Best and Worst Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14809f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_results:\n",
    "    # Sort by overall score\n",
    "    sorted_results = sorted(\n",
    "        successful_results,\n",
    "        key=lambda x: x['aggregate_metrics']['avg_overall_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"Top 5 Best Performers:\")\n",
    "    for i, r in enumerate(sorted_results[:5], 1):\n",
    "        score = r['aggregate_metrics']['avg_overall_score']\n",
    "        translit = r['ground_truth']['transliteration']\n",
    "        print(f\"  {i}. {r['inscription_id']}: {score:.3f}\")\n",
    "        print(f\"     Ground truth: {translit}\")\n",
    "    \n",
    "    print(\"\\nTop 5 Worst Performers:\")\n",
    "    for i, r in enumerate(sorted_results[-5:][::-1], 1):\n",
    "        score = r['aggregate_metrics']['avg_overall_score']\n",
    "        translit = r['ground_truth']['transliteration']\n",
    "        print(f\"  {i}. {r['inscription_id']}: {score:.3f}\")\n",
    "        print(f\"     Ground truth: {translit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c66e7",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d690e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['save_results']:\n",
    "    # Create results directory\n",
    "    CONFIG['results_dir'].mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"batch_eval_{CONFIG['prompt_template']}_{timestamp}.json\"\n",
    "    output_path = CONFIG['results_dir'] / filename\n",
    "    \n",
    "    # Prepare output data\n",
    "    output = {\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'model': 'llama3.2-vision',\n",
    "            'num_inscriptions': len(CONFIG['inscription_ids']),\n",
    "            'prompt_template': CONFIG['prompt_template'],\n",
    "            'config': {k: str(v) for k, v in CONFIG.items() if k != 'results_dir'},\n",
    "            'duration_seconds': elapsed_time,\n",
    "        },\n",
    "        'summary': {\n",
    "            'total_inscriptions': len(results),\n",
    "            'successful': len(successful_results),\n",
    "            'failed': len(failed_results),\n",
    "            'avg_overall_score': float(np.mean(scores)) if scores else 0,\n",
    "            'avg_character_similarity': float(np.mean(char_sims)) if char_sims else 0,\n",
    "            'avg_word_accuracy': float(np.mean(word_accs)) if word_accs else 0,\n",
    "        },\n",
    "        'results': results,\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {output_path}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"Results not saved (CONFIG['save_results'] = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed327334",
   "metadata": {},
   "source": [
    "## Export Summary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eeff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for easier analysis\n",
    "if successful_results:\n",
    "    summary_data = []\n",
    "    for r in successful_results:\n",
    "        summary_data.append({\n",
    "            'inscription_id': r['inscription_id'],\n",
    "            'transliteration': r['ground_truth']['transliteration'],\n",
    "            'transliteration_length': r['ground_truth']['transliteration_length'],\n",
    "            'overall_score': r['aggregate_metrics']['avg_overall_score'],\n",
    "            'character_similarity': r['aggregate_metrics']['avg_character_similarity'],\n",
    "            'word_accuracy': r['aggregate_metrics']['avg_word_accuracy'],\n",
    "            'num_images_tested': r['num_images_tested'],\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display\n",
    "    print(\"\\nSummary DataFrame:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if CONFIG['save_results']:\n",
    "        csv_path = CONFIG['results_dir'] / f\"summary_{CONFIG['prompt_template']}_{timestamp}.csv\"\n",
    "        summary_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✓ Summary CSV saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92b86a",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Analyze the detailed results JSON file\n",
    "- Compare performance across different prompt templates\n",
    "- Test on larger subsets of inscriptions\n",
    "- Investigate worst-performing cases\n",
    "- Try different temperature settings\n",
    "- Test multi-image analysis strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbc5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
